{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list ids: \n",
    "ids=['00000']\n",
    "\n",
    "for i in range(1,27639):\n",
    "    num=''\n",
    "    if i <10:\n",
    "        num+='0000'\n",
    "        num+=str(i)\n",
    "    elif i<100:\n",
    "        num+='000'\n",
    "        num+=str(i)        \n",
    "    elif i<1000:\n",
    "        num+='00'\n",
    "        num+=str(i)       \n",
    "    elif i<10000:\n",
    "        num+='0'\n",
    "        num+=str(i)  \n",
    "    else:\n",
    "        num+=str(i)\n",
    "    ids.append(num)\n",
    "#print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline to transform an image\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch import Tensor\n",
    "import torch.utils\n",
    "import numpy as np\n",
    "\n",
    "scaler=transforms.Resize((224,224))\n",
    "normalize=transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor=transforms.ToTensor()\n",
    "\n",
    "def transform_image(image_id):\n",
    "    image=Image.open(\"Yummly28K\\images27638\\img\" + image_id +'.jpg')\n",
    "    t_img=normalize(to_tensor(scaler(image))).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    return t_img.cpu().detach().numpy()\n",
    "\n",
    "transform_image('00001').shape\n",
    "#type(transform_image('00001'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"one_hot_meat.csv\", 'r')\n",
    "l = file.readlines()\n",
    "file.close()\n",
    "one_hot = []\n",
    "dummy = [1] * 12\n",
    "one_hot.append(dummy)\n",
    "for item in l:\n",
    "    temp = item.strip('\\n')\n",
    "    temp = temp.split(',')\n",
    "    n = len(temp)\n",
    "    for i in range(n):\n",
    "        temp[i] = int(temp[i])\n",
    "    one_hot.append(temp)\n",
    "\n",
    "#print(one_hot[0], one_hot[1])\n",
    "# index 1 is 1st y hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "X_train_normalized_list=[]\n",
    "y_train_list=[]\n",
    "\n",
    "X_val_normalized_list=[]\n",
    "y_val_list=[]\n",
    "\n",
    "for i in range(1,2501):\n",
    "    image=transform_image(ids[i])\n",
    "    X_train_normalized_list.append(image)\n",
    "    y_train_list.append(one_hot[i])\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "\n",
    "for i in range(2501, 3001):\n",
    "    image=transform_image(ids[i])\n",
    "    X_val_normalized_list.append(image)\n",
    "    y_val_list.append(one_hot[i])\n",
    "    if i%100==0:\n",
    "        print(i)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "completed\n",
      "clear memory\n"
     ]
    }
   ],
   "source": [
    "n_iterations=40\n",
    "batch_size=250\n",
    "\n",
    "X_train_normalized=np.array(X_train_normalized_list) \n",
    "print(\"completed\")\n",
    "y_train=np.array(y_train_list)\n",
    "print(\"completed\")\n",
    "#rint(X_train_normalized.shape())\n",
    "#print(y_train_normalized.shape())\n",
    "\n",
    "X_val_normalized=np.array(X_val_normalized_list)    \n",
    "y_val=np.array(y_val_list)\n",
    "\n",
    "n_data=X_train_normalized.shape[0]\n",
    "n_batch=int(np.ceil(n_data/batch_size))\n",
    "\n",
    "X_train_tensor=Tensor(X_train_normalized)\n",
    "X_val_tensor=Tensor(X_val_normalized)\n",
    "X_train_normalized =[]\n",
    "X_val_normalized = []\n",
    "print(\"clear memory\")\n",
    "\n",
    "y_train_tensor=Tensor(y_train).float()\n",
    "y_val_tensor=Tensor(y_val).float()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2500, 1, 3, 224, 224])\n",
      "torch.Size([500, 1, 3, 224, 224])\n",
      "torch.Size([2500, 12])\n",
      "torch.Size([500, 12])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(X_val_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "print(y_val_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "use_gpu=torch.cuda.is_available()\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,epochs=5):\n",
    "    best_model_w=copy.deepcopy(model.state_dict())\n",
    "    best_accuracy=0.0\n",
    "    worst_x_index = 0\n",
    "    worst_prediction = []\n",
    "    worst_acccuracy = 1.0\n",
    "    y_actual = []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        for phase in ['train','valid']:\n",
    "            if phase=='train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "\n",
    "            if phase=='train':\n",
    "                for i in range(0,2500):\n",
    "                    if i % 100 == 0:\n",
    "                        print(i)\n",
    "                    inputs=X_train_tensor[i]\n",
    "                    labels=y_train_tensor[i]\n",
    "\n",
    "                    if use_gpu==True:\n",
    "                        inputs=Variable(inputs.cuda())\n",
    "                        labels=Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs),Variable(labels)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs=model(inputs)\n",
    "                    #print(\"this is outputs:\", outputs[0])\n",
    "                    #print(\"this is labels:\", labels)\n",
    "                    loss=criterion(outputs[0], labels)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss+=(loss.data.mean())\n",
    "                    predict = make_prediction(outputs[0])\n",
    "                    acc = check_correctness(predict,labels)\n",
    "                    train_accuracy += acc\n",
    "                    #if acc < worst_acccuracy:\n",
    "                    #    worst_x_index = i\n",
    "                    #    worst_prediction = predict\n",
    "                    #    y_actual = labels\n",
    "                    if acc == 1 and labels[-1] != 1:\n",
    "                        worst_x_index = i\n",
    "                        worst_predicion = 0\n",
    "                        y_actual = labels\n",
    "                \n",
    "                train_accuracy = train_accuracy/2500\n",
    "                if train_accuracy > best_accuracy:\n",
    "                    best_accuracy = train_accuracy\n",
    "                    best_model_w = copy.deepcopy(model.state_dict())\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                for j in range(0, 500):\n",
    "                    #print(j)\n",
    "                    inputs=X_val_tensor[j]\n",
    "                    labels=y_val_tensor[j]\n",
    "                    \n",
    "                    \n",
    "                    if use_gpu==True:\n",
    "                            inputs=Variable(inputs.cuda())\n",
    "                            labels=Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs),Variable(labels)\n",
    "                    outputs=model(inputs)\n",
    "                    loss=criterion(outputs[0],labels)\n",
    "                    val_loss+=(loss.data.mean())\n",
    "                    predict = make_prediction(outputs[0])\n",
    "                    val_accuracy += check_correctness(predict, labels)\n",
    "                    \n",
    "                val_accuracy = val_accuracy/500\n",
    "                \n",
    "        print(\"For Epoch number:\", epoch, \"Phase: training, total loss: \", train_loss, \"Average_Accuracy:\", train_accuracy)\n",
    "        print(\"For Epoch number:\", epoch, \"Phase: validation, total loss: \", val_loss, \"Average_Accuracy:\", val_accuracy)\n",
    "\n",
    "    \n",
    "    print(\"the worst one is located at: \", worst_x_index)\n",
    "    print(\"the guess was:\", worst_prediction)\n",
    "    print(\"The actual was:\" , y_actual)\n",
    "    model.load_state_dict(best_model_w)\n",
    "    return model\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_prediction(tensor):\n",
    "    n = len(tensor)\n",
    "    status=False\n",
    "    for i in range(n):\n",
    "        if tensor[i] > .5:\n",
    "            tensor[i] = 1\n",
    "            status=True\n",
    "        elif tensor[i] < .5:\n",
    "            tensor[i] = 0\n",
    "        else:\n",
    "            tensor[i] = random.randint(0,1)\n",
    "            \n",
    "    if (status == False):\n",
    "        tensor[-1] = 1\n",
    "            \n",
    "    return tensor\n",
    "    \n",
    "def check_correctness(y_pred, y_actual):\n",
    "    n = len(y_pred)\n",
    "    nb_correct = 0\n",
    "    nb_incorrect = 0\n",
    "    nb_target = 0\n",
    "    for i in range(n):\n",
    "        if y_actual[i] == 1:\n",
    "            if y_pred[i] == 1:\n",
    "                nb_correct += 1\n",
    "            #false negative\n",
    "            else:\n",
    "                nb_incorrect += 1\n",
    "            nb_target += 1\n",
    "        #y_pred == 0\n",
    "        else:\n",
    "            #false positive\n",
    "            if y_pred[i] == 1:\n",
    "                nb_incorrect += 1\n",
    "        \n",
    "    accuracy = nb_correct/(nb_correct + nb_incorrect)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "resnet50 = models.resnet50()\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "## will need to replace last layer of model into fully conneted softmax\n",
    "#  last layer of resnet (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 12 for length 12 of meat \n",
    "#model.fc=torch.nn.Sequential(torch.nn.Linear(in_features=2048, out_features=12, bias=True),torch.nn.Softmax())\n",
    "\n",
    "state_dic=torch.utils.model_zoo.load_url(\"https://download.pytorch.org/models/resnet50-19c8e357.pth\", progress=True)\n",
    "model.load_state_dict(state_dic)\n",
    "model.eval()\n",
    "\n",
    "#resnet50_weights=copy.deepcopy(state_dic)\n",
    "\n",
    "\n",
    "#print(state_dic)\n",
    "#print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_gpu=torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "#I recommend training with these layers unfrozen for a couple of epochs after the initial frozen training\n",
    "count = 0\n",
    "retrain = 125\n",
    "for param in model.parameters():\n",
    "    if count < retrain:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "    count +=1\n",
    "numb_ftrs = model.fc.in_features\n",
    "model.fc=torch.nn.Linear(in_features=numb_ftrs, out_features=12, bias=True)\n",
    "\n",
    "#num_ftrs = model.fc.in_features\n",
    "#model.fc = torch.nn.Linear(num_ftrs, len(classes))\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=.001)\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#loaders = {'train':train_loader, 'valid':valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "36\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "t = 0\n",
    "f = 0\n",
    "for param in model.parameters():\n",
    "    count += 1\n",
    "    if param.requires_grad == True:\n",
    "        t += 1\n",
    "    else:\n",
    "        f += 1\n",
    "print(count)\n",
    "print(t)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "For Epoch number: 0 Phase: training, total loss:  tensor(2131.7910, device='cuda:0') Average_Accuracy: 0.4706466666666666\n",
      "For Epoch number: 0 Phase: validation, total loss:  tensor(237.4136, device='cuda:0') Average_Accuracy: 0.602\n",
      "Epoch 1/4\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "For Epoch number: 1 Phase: training, total loss:  tensor(2058.8735, device='cuda:0') Average_Accuracy: 0.4709\n",
      "For Epoch number: 1 Phase: validation, total loss:  tensor(236.9566, device='cuda:0') Average_Accuracy: 0.592\n",
      "Epoch 2/4\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "For Epoch number: 2 Phase: training, total loss:  tensor(2041.2578, device='cuda:0') Average_Accuracy: 0.4761333333333334\n",
      "For Epoch number: 2 Phase: validation, total loss:  tensor(253.1236, device='cuda:0') Average_Accuracy: 0.573\n",
      "Epoch 3/4\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "For Epoch number: 3 Phase: training, total loss:  tensor(2029.9502, device='cuda:0') Average_Accuracy: 0.4781666666666667\n",
      "For Epoch number: 3 Phase: validation, total loss:  tensor(260.8972, device='cuda:0') Average_Accuracy: 0.5403333333333333\n",
      "Epoch 4/4\n",
      "----------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "For Epoch number: 4 Phase: training, total loss:  tensor(1962.3937, device='cuda:0') Average_Accuracy: 0.4833333333333334\n",
      "For Epoch number: 4 Phase: validation, total loss:  tensor(327.5798, device='cuda:0') Average_Accuracy: 0.4663333333333334\n",
      "the worst one is located at:  2469\n",
      "the guess was: []\n",
      "The actual was: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# make test data\n",
    "# take \n",
    "X_test_normalized_list=[]\n",
    "y_test_list=[]\n",
    "\n",
    "for i in range(3001, 3501):\n",
    "    image=transform_image(ids[i])\n",
    "    X_test_normalized_list.append(image)\n",
    "    y_test_list.append(one_hot[i])  \n",
    "        \n",
    "X_test_normalized=np.array(X_test_normalized_list)\n",
    "\n",
    "y_test=np.array(y_test_list)\n",
    "#y_test_tensor=Tensor(y_test).float()\n",
    "\n",
    "data=Tensor(X_test_normalized)\n",
    "actual=Tensor(y_test).float()\n",
    "\n",
    "sum_acc=0\n",
    "for i in range(500):\n",
    "    inputs=data[i].cuda()\n",
    "    output=model(inputs)\n",
    "    prediction=make_prediction(output[0])\n",
    "    actuals=actual[i].cuda()\n",
    "    acc=check_correctness(prediction,actuals)\n",
    "    sum_acc+=acc\n",
    "\n",
    "\n",
    "overall_acc=sum_acc/500\n",
    "print(overall_acc)\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(img_id):\n",
    "    img = transform_image(img_id)\n",
    "    x = Tensor(img)\n",
    "    y_val = one_hot[int(img_id)]\n",
    "    y_val = Tensor(y_val)\n",
    "    if use_gpu==True:\n",
    "        inputs=Variable(x.cuda())\n",
    "        labels=Variable(y_val.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(x.cuda()),Variable(y_val.cuda())\n",
    "    result = model(inputs)\n",
    "    prediction = make_prediction(result[0])\n",
    "        \n",
    "    print(y_val)\n",
    "    print(prediction)\n",
    "test_1(\"02496\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
